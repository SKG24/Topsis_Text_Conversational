{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FQhbQyYwSUOB",
        "outputId": "29d65f57-cd8d-467c-d85d-9cdedd6e3ec8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.metrics import f1_score\n",
        "from transformers import AutoModelForCausalLM, AutoModelForSeq2SeqLM, AutoTokenizer\n",
        "from time import time\n",
        "import torch\n",
        "\n",
        "# Example list of models\n",
        "models = {\n",
        "    \"GPT2\": \"gpt2\",\n",
        "    \"T5\": \"t5-small\",  # T5 model needs AutoModelForSeq2SeqLM\n",
        "    \"BART\": \"facebook/bart-large\"  # BART can be used with AutoModelForSeq2SeqLM as well\n",
        "}\n",
        "\n",
        "# Test prompts for generating responses\n",
        "test_prompts = [\n",
        "    \"What's the weather like today?\",\n",
        "    \"Tell me a joke.\",\n",
        "    \"How are you?\",\n",
        "    \"Who won the world series in 2020?\",\n",
        "    \"What's the capital of France?\"\n",
        "]\n",
        "\n",
        "# Function to evaluate each model based on given criteria\n",
        "def evaluate_model(model_name, model_id):\n",
        "    # Dynamically choose the correct model class\n",
        "    if \"t5\" in model_id or \"bart\" in model_id:  # For T5 and BART\n",
        "        model_class = AutoModelForSeq2SeqLM\n",
        "    else:  # For causal language models like GPT-2, GPT-3, etc.\n",
        "        model_class = AutoModelForCausalLM\n",
        "\n",
        "    # Load the model and tokenizer\n",
        "    model = model_class.from_pretrained(model_id)\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "\n",
        "    # Start the evaluation for this model\n",
        "    start_time = time()\n",
        "\n",
        "    # Generate responses for each test prompt and measure time\n",
        "    total_time = 0\n",
        "    responses = []\n",
        "    for prompt in test_prompts:\n",
        "        inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
        "        with torch.no_grad():\n",
        "            outputs = model.generate(inputs['input_ids'], max_length=50)\n",
        "        response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "        responses.append(response)\n",
        "        total_time += time() - start_time  # measure inference time\n",
        "\n",
        "    # Measure inference time (seconds per response)\n",
        "    avg_response_time = total_time / len(test_prompts)\n",
        "\n",
        "    # Dummy metrics for the sake of this example\n",
        "    # In a real scenario, you should evaluate the accuracy of the model on a real dataset.\n",
        "    accuracy = np.random.uniform(0.85, 0.95)  # Simulated accuracy for demonstration\n",
        "\n",
        "    # Calculate F1 score based on dummy predicted and true labels\n",
        "    true_labels = [\n",
        "        \"I'm good, thank you!\", \"My name is GPT.\", \"Why don't scientists trust atoms? Because they make up everything!\",\n",
        "        \"42\", \"AI is the simulation of human intelligence in machines.\"\n",
        "    ]\n",
        "    predicted_labels = responses\n",
        "\n",
        "    f1 = f1_score(true_labels, predicted_labels, average='weighted', zero_division=1)\n",
        "\n",
        "    # Simulate memory usage (MB) based on model size (just for demonstration)\n",
        "    model_size = model.num_parameters() / 1e6  # Convert from parameters to MB approximation\n",
        "    memory_usage = model_size\n",
        "\n",
        "    # Simulate diversity score based on the variety of responses\n",
        "    diversity_score = np.random.uniform(0.7, 1.0)  # This would require a more complex evaluation\n",
        "\n",
        "    return accuracy, avg_response_time, memory_usage, f1, diversity_score\n",
        "\n",
        "# Initialize lists to store the scores for each model\n",
        "accuracy_scores = []\n",
        "speed_scores = []\n",
        "memory_scores = []\n",
        "f1_scores = []\n",
        "diversity_scores = []\n",
        "\n",
        "# Loop over each model and collect their performance metrics\n",
        "for model_name, model_id in models.items():\n",
        "    accuracy, speed, memory, f1, diversity = evaluate_model(model_name, model_id)\n",
        "    accuracy_scores.append(accuracy)\n",
        "    speed_scores.append(speed)\n",
        "    memory_scores.append(memory)\n",
        "    f1_scores.append(f1)\n",
        "    diversity_scores.append(diversity)\n",
        "\n",
        "# Collect the results in a table format\n",
        "results_df = pd.DataFrame({\n",
        "    \"Model\": list(models.keys()),\n",
        "    \"Accuracy\": accuracy_scores,\n",
        "    \"Response Speed (s)\": speed_scores,\n",
        "    \"Memory Usage (MB)\": memory_scores,\n",
        "    \"F1 Score\": f1_scores,\n",
        "    \"Diversity Score\": diversity_scores\n",
        "})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "BznpQyTESU2u",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9a6d5e01-b21d-4b9f-a75e-4f62304bb851"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Model  TOPSIS Score  Rank\n",
            "0  GPT2      0.525058   2.0\n",
            "1    T5      0.154582   3.0\n",
            "2  BART      0.581161   1.0\n"
          ]
        }
      ],
      "source": [
        "# Normalize the scores (apply MinMax scaling between 0 and 1)\n",
        "scaler = MinMaxScaler()\n",
        "normalized_df = results_df.copy()\n",
        "normalized_df.iloc[:, 1:] = scaler.fit_transform(results_df.iloc[:, 1:])\n",
        "\n",
        "# Define the ideal and negative ideal solutions\n",
        "ideal_solution = normalized_df.drop(\"Model\", axis=1).max()  # Ideal solution (max value for each metric)\n",
        "negative_ideal_solution = normalized_df.drop(\"Model\", axis=1).min()  # Negative ideal solution (min value for each metric)\n",
        "\n",
        "# Convert ideal_solution and negative_ideal_solution to numpy arrays for broadcasting\n",
        "ideal_solution = ideal_solution.values\n",
        "negative_ideal_solution = negative_ideal_solution.values\n",
        "\n",
        "# Calculate the Euclidean distance to the ideal and negative ideal solutions\n",
        "distance_to_ideal = np.sqrt(((normalized_df.drop(\"Model\", axis=1).values - ideal_solution) ** 2).sum(axis=1))\n",
        "distance_to_negative_ideal = np.sqrt(((normalized_df.drop(\"Model\", axis=1).values - negative_ideal_solution) ** 2).sum(axis=1))\n",
        "\n",
        "# Calculate the TOPSIS score\n",
        "topsis_score = distance_to_negative_ideal / (distance_to_ideal + distance_to_negative_ideal)\n",
        "\n",
        "# Add the TOPSIS score to the DataFrame\n",
        "normalized_df[\"TOPSIS Score\"] = topsis_score\n",
        "\n",
        "# Rank the models based on the TOPSIS score\n",
        "normalized_df[\"Rank\"] = normalized_df[\"TOPSIS Score\"].rank(ascending=False)\n",
        "\n",
        "# Display the final results with ranking\n",
        "print(normalized_df[[\"Model\", \"TOPSIS Score\", \"Rank\"]])"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DBN-P0F8ebL2"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "gpuType": "V28",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}